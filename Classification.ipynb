{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.3.0\n",
      "Python version:  3.8.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM\n",
      "Epoch 1/5\n",
      "752/752 - 34s - loss: 0.9058 - accuracy: 0.5462 - val_loss: 0.8322 - val_accuracy: 0.6168\n",
      "Epoch 2/5\n",
      "752/752 - 34s - loss: 0.8096 - accuracy: 0.6240 - val_loss: 0.8030 - val_accuracy: 0.6494\n",
      "Epoch 3/5\n",
      "752/752 - 34s - loss: 0.7566 - accuracy: 0.6552 - val_loss: 0.7861 - val_accuracy: 0.6565\n",
      "Epoch 4/5\n",
      "752/752 - 34s - loss: 0.7066 - accuracy: 0.6835 - val_loss: 0.7865 - val_accuracy: 0.6542\n",
      "Epoch 5/5\n",
      "752/752 - 34s - loss: 0.6616 - accuracy: 0.7087 - val_loss: 0.8779 - val_accuracy: 0.6270\n",
      "1410/1410 - 12s - loss: 0.5970 - accuracy: 0.7411\n",
      "111/111 - 1s - loss: 0.8779 - accuracy: 0.6270\n",
      "LSTM Training Accuracy:  [0.5970451831817627, 0.7410922050476074]\n",
      "LSTM Test Accuracy:  [0.8779105544090271, 0.6270178556442261]\n",
      "Epoch 1/5\n",
      "752/752 - 35s - loss: 0.9015 - accuracy: 0.5555 - val_loss: 0.7531 - val_accuracy: 0.6411\n",
      "Epoch 2/5\n",
      "752/752 - 34s - loss: 0.8078 - accuracy: 0.6272 - val_loss: 0.7419 - val_accuracy: 0.6303\n",
      "Epoch 3/5\n",
      "752/752 - 34s - loss: 0.7533 - accuracy: 0.6563 - val_loss: 0.7731 - val_accuracy: 0.6481\n",
      "Epoch 4/5\n",
      "752/752 - 34s - loss: 0.7072 - accuracy: 0.6828 - val_loss: 0.7593 - val_accuracy: 0.6346\n",
      "Epoch 5/5\n",
      "752/752 - 35s - loss: 0.6609 - accuracy: 0.7077 - val_loss: 0.8125 - val_accuracy: 0.6147\n",
      "1410/1410 - 12s - loss: 0.6346 - accuracy: 0.7328\n",
      "58/58 - 0s - loss: 0.8125 - accuracy: 0.6147\n",
      "LSTM Training Accuracy:  [0.6345832347869873, 0.7327775359153748]\n",
      "LSTM Test Accuracy:  [0.8124625086784363, 0.6146789193153381]\n",
      "Epoch 1/5\n",
      "752/752 - 38s - loss: 0.8981 - accuracy: 0.5598 - val_loss: 0.8783 - val_accuracy: 0.5784\n",
      "Epoch 2/5\n",
      "752/752 - 38s - loss: 0.8071 - accuracy: 0.6254 - val_loss: 0.8648 - val_accuracy: 0.6225\n",
      "Epoch 3/5\n",
      "752/752 - 37s - loss: 0.7551 - accuracy: 0.6587 - val_loss: 0.8582 - val_accuracy: 0.5923\n",
      "Epoch 4/5\n",
      "752/752 - 38s - loss: 0.7060 - accuracy: 0.6840 - val_loss: 0.9250 - val_accuracy: 0.5834\n",
      "Epoch 5/5\n",
      "752/752 - 37s - loss: 0.6611 - accuracy: 0.7097 - val_loss: 0.9884 - val_accuracy: 0.5612\n",
      "1410/1410 - 14s - loss: 0.6459 - accuracy: 0.7051\n",
      "75/75 - 1s - loss: 0.9884 - accuracy: 0.5612\n",
      "LSTM Training Accuracy:  [0.6458730697631836, 0.7050841450691223]\n",
      "LSTM Test Accuracy:  [0.9884281158447266, 0.561160147190094]\n",
      "Training Logistic_Regression\n",
      "Logistic Regression Accuracy Score:  0.6510903426791277\n",
      "Logistic Regression Accuracy Score:  0.6524554776038856\n",
      "Logistic Regression Accuracy Score:  0.6174863387978142\n",
      "Training BiLSTM\n",
      "Epoch 1/5\n",
      "705/705 - 672s - loss: 0.8815 - accuracy: 0.5815 - val_loss: 0.7651 - val_accuracy: 0.6695\n",
      "Epoch 2/5\n",
      "705/705 - 695s - loss: 0.7014 - accuracy: 0.6946 - val_loss: 0.7904 - val_accuracy: 0.6511\n",
      "Epoch 3/5\n",
      "705/705 - 702s - loss: 0.6282 - accuracy: 0.7355 - val_loss: 0.8075 - val_accuracy: 0.6562\n",
      "Epoch 4/5\n",
      "705/705 - 728s - loss: 0.5695 - accuracy: 0.7600 - val_loss: 0.8512 - val_accuracy: 0.6392\n",
      "1410/1410 - 78s - loss: 0.4468 - accuracy: 0.8224\n",
      "111/111 - 5s - loss: 0.8512 - accuracy: 0.6392\n",
      "BiLSTM Training Accuracy:  [0.4467681646347046, 0.8223764300346375]\n",
      "BiLSTM Test Accuracy [0.8512383699417114, 0.6391956806182861]\n",
      "Epoch 1/5\n",
      "705/705 - 767s - loss: 0.8830 - accuracy: 0.5843 - val_loss: 0.7377 - val_accuracy: 0.6525\n",
      "Epoch 2/5\n",
      "705/705 - 749s - loss: 0.7034 - accuracy: 0.6942 - val_loss: 0.7424 - val_accuracy: 0.6400\n",
      "Epoch 3/5\n",
      "705/705 - 760s - loss: 0.6277 - accuracy: 0.7334 - val_loss: 0.7591 - val_accuracy: 0.6508\n",
      "Epoch 4/5\n",
      "705/705 - 746s - loss: 0.5663 - accuracy: 0.7603 - val_loss: 0.8454 - val_accuracy: 0.6325\n",
      "1410/1410 - 76s - loss: 0.4264 - accuracy: 0.8262\n",
      "58/58 - 3s - loss: 0.8454 - accuracy: 0.6325\n",
      "BiLSTM Training Accuracy:  [0.4263737201690674, 0.8261679410934448]\n",
      "BiLSTM Test Accuracy [0.8454458117485046, 0.6324878334999084]\n",
      "Epoch 1/5\n",
      "705/705 - 760s - loss: 0.9017 - accuracy: 0.5657 - val_loss: 0.8448 - val_accuracy: 0.5923\n",
      "Epoch 2/5\n",
      "705/705 - 725s - loss: 0.7021 - accuracy: 0.6947 - val_loss: 0.8447 - val_accuracy: 0.6221\n",
      "Epoch 3/5\n",
      "705/705 - 748s - loss: 0.6243 - accuracy: 0.7342 - val_loss: 0.8496 - val_accuracy: 0.6196\n",
      "Epoch 4/5\n",
      "705/705 - 718s - loss: 0.5646 - accuracy: 0.7627 - val_loss: 0.9343 - val_accuracy: 0.5965\n",
      "Epoch 5/5\n",
      "705/705 - 717s - loss: 0.5071 - accuracy: 0.7849 - val_loss: 1.1148 - val_accuracy: 0.6024\n",
      "1410/1410 - 55s - loss: 0.3793 - accuracy: 0.8404\n",
      "75/75 - 3s - loss: 1.1148 - accuracy: 0.6024\n",
      "BiLSTM Training Accuracy:  [0.37927234172821045, 0.8404026627540588]\n",
      "BiLSTM Test Accuracy [1.1147822141647339, 0.6023539304733276]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import testsets\n",
    "import evaluation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Dropout, Activation, Embedding, MaxPooling1D, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from platform import python_version\n",
    "\n",
    "print(\"tensorflow version: \",tf.__version__)\n",
    "print(\"Python version: \",python_version())\n",
    "\n",
    "train_data =[]\n",
    "X_train = []\n",
    "y_train = []\n",
    "embeddings_index = dict()\n",
    "embedding_matrix = []\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def preprocess(data):\n",
    "    processed_text = data.lower() # Convert to lowercase\n",
    "    processed_text = re.sub('@[^\\s]+','',processed_text) # Replace user mentions \n",
    "    processed_text = re.sub('(www\\.[^\\s]+)|(http[^\\s]+)|(ftp://[^\\s]+)','',processed_text) # Remove URL's\n",
    "    processed_text = re.sub('[^a-z\\s]','',processed_text) # Remove special characters, numbers, punctuations\n",
    "    processed_text = re.sub(r'\\b[a-z]{1,2}\\b','',processed_text) # Remove words with length less than 3\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokenized_tweet =  word_tokenize(processed_text)\n",
    "    processed_text = [w for w in tokenized_tweet if not w in stop_words] # Remove stop words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    pos_tagged_sentence = pos_tag(processed_text)\n",
    "    for word, tag in pos_tagged_sentence:\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return ' '.join(lemmatized_sentence)\n",
    "\n",
    "# Pre trained word embedding\n",
    "with open('glove.6B.100d.txt',encoding=\"utf8\") as g:\n",
    "    data = g.read().splitlines()\n",
    "    for line in data:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coef = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coef\n",
    "\n",
    "        \n",
    "# load training data\n",
    "with open(\"twitter-training-data.txt\",encoding=\"utf8\") as f:\n",
    "    data = f.read().splitlines()\n",
    "    for row in data:\n",
    "        tweet = row.split(\"\\t\")\n",
    "        if len(tweet) == 3:\n",
    "            preprocessed_text = preprocess(tweet[2])\n",
    "            train_data.append([preprocessed_text,tweet[1]])\n",
    "    train_data = pd.DataFrame(train_data) \n",
    "    train_data.columns = [\"Text\", \"Sentiment\"]\n",
    "X_train = train_data['Text'].values\n",
    "y_train = train_data['Sentiment'].values\n",
    "\n",
    "\n",
    "for classifier in ['LSTM','Logistic_Regression','BiLSTM']: \n",
    "    if classifier == 'LSTM':\n",
    "        print('Training ' + classifier)\n",
    "        text_tokenizer_LSTM = Tokenizer(num_words=5000, oov_token=\"oov\")\n",
    "        text_tokenizer_LSTM.fit_on_texts(X_train)\n",
    "        word_index = text_tokenizer_LSTM.word_index\n",
    "        embedding_dim = 100 \n",
    "        embedding_matrix = np.zeros((5000, embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            if i < 5000:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        x_train = text_tokenizer_LSTM.texts_to_sequences(X_train)\n",
    "        x_train = pad_sequences(x_train, padding='post', maxlen=100)\n",
    "        Y_train = pd.get_dummies(y_train)\n",
    "\n",
    "    elif classifier == 'Logistic_Regression':\n",
    "        print('Training ' + classifier)\n",
    "        tfidf_vectorizer =  TfidfVectorizer(max_features=3000, min_df=7, max_df=0.7,ngram_range=(1,3),smooth_idf=False)\n",
    "        x_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "        lr = LogisticRegression(penalty='l1',C = 1000,random_state = 0,solver='liblinear',max_iter=10000)\n",
    "        lr.fit(x_train.toarray(),y_train)\n",
    "        \n",
    "    elif classifier == 'BiLSTM':\n",
    "        print('Training ' + classifier)\n",
    "        text_tokenizer = Tokenizer(num_words=20000, oov_token=\"oov\")\n",
    "        text_tokenizer.fit_on_texts(X_train)\n",
    "        x_train = text_tokenizer.texts_to_sequences(X_train)\n",
    "        x_train = pad_sequences(x_train, padding='post', maxlen=200)\n",
    "        label_tokenizer = Tokenizer()\n",
    "        label_tokenizer.fit_on_texts(y_train)\n",
    "        Y_train = np.array(label_tokenizer.texts_to_sequences(y_train))\n",
    "\n",
    "    for testset in testsets.testsets:\n",
    "        # TODO: classify tweets in test set\n",
    "        test_data = []\n",
    "        y_pred = 0\n",
    "        with open(testset,encoding=\"utf8\") as t:          \n",
    "            data = t.read().splitlines()\n",
    "            for row in data:\n",
    "                tweet = row.split(\"\\t\")\n",
    "                if len(tweet) == 3:\n",
    "                    preprocessed_text = preprocess(tweet[2])\n",
    "                    test_data.append([preprocessed_text,tweet[0],tweet[1]])\n",
    "        test_data = pd.DataFrame(test_data) \n",
    "        test_data.columns = [\"Text\",\"Tweet_Id\",\"Sentiment\"]\n",
    "        X_test = test_data['Text'].values\n",
    "        \n",
    "        if(classifier == 'LSTM'):\n",
    "            x_test = text_tokenizer_LSTM.texts_to_sequences(X_test)\n",
    "            x_test = pad_sequences(x_test, padding='post', maxlen=100)\n",
    "            y_test = pd.get_dummies(test_data['Sentiment']).values\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(5000, 100,weights=[embedding_matrix],trainable = False))\n",
    "            model.add(Conv1D(100, 5, activation='relu'))\n",
    "            model.add(MaxPooling1D(pool_size = 4))\n",
    "            model.add(LSTM(100))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(3, activation='softmax'))\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "            model.fit(x_train, Y_train, epochs=5, batch_size=60, verbose = 2,validation_data=(x_test, y_test))\n",
    "            train_acc = model.evaluate(x_train, Y_train, verbose=2)\n",
    "            test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"LSTM Training Accuracy: \",train_acc)\n",
    "            print(\"LSTM Test Accuracy: \",test_acc)    \n",
    "            \n",
    "        elif(classifier == 'Logistic_Regression'):\n",
    "            x_test= tfidf_vectorizer.transform(X_test)\n",
    "            y_pred = lr.predict(x_test.toarray())\n",
    "            print(\"Logistic Regression Accuracy Score: \",accuracy_score(test_data['Sentiment'].values, y_pred))\n",
    "            \n",
    "        elif(classifier == 'BiLSTM'):\n",
    "            x_test = text_tokenizer.texts_to_sequences(X_test)\n",
    "            x_test = pad_sequences(x_test, padding='post', maxlen=200)\n",
    "            y_test = np.array(label_tokenizer.texts_to_sequences(test_data['Sentiment'].values))\n",
    "            vocab = len(text_tokenizer.word_index) + 1\n",
    "            model = tf.keras.Sequential([    \n",
    "                tf.keras.layers.Embedding(vocab, 100),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(80,dropout=0.5, recurrent_dropout=0.5)),\n",
    "                tf.keras.layers.Dense(80, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ])   \n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(x_train, Y_train, epochs=5, batch_size=64, verbose = 2,validation_data=(x_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "            train_acc = model.evaluate(x_train, Y_train, verbose=2)\n",
    "            test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"BiLSTM Training Accuracy: \",train_acc)\n",
    "            print(\"BiLSTM Test Accuracy\",test_acc)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
